#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Apr  9 07:59:17 2021

@author: hien
"""
from __future__ import division
import random
from src.environment import Environment
from src.agents import Agent
from read_input import Data
from itertools import count
import numpy as np
from collections import deque
from sklearn.utils import shuffle
import time
import torch
from src.utils import plot, dotdict

cargs = dotdict({
    'visualize': True,
    'min_size': 7,
    'max_size': 7,
    'n_epochs': 100,
    'show_screen': True,
})

args = [
        dotdict({
            'optimizer': 'adas',
            'lr': 1e-4,
            'exp_rate': 0.0,
            'gamma': 0.99,
            'tau': 0.01,
            'max_grad_norm': 0.3,
            'discount': 0.6,
            'num_channels': 64,
            'batch_size': 256,
            'replay_memory_size': 100000,
            'dropout': 0.6,
            'initial_epsilon': 0.1,
            'final_epsilon': 1e-4,
            'dir': './Models/',
            'load_checkpoint': True,
            'saved_checkpoint': True
        }),
        
        dotdict({
            'optimizer': 'adas',
            'lr': 1e-4,
            'exp_rate': 0.0,
            'gamma': 0.99,
            'tau': 0.01,
            'max_grad_norm': 0.3,
            'discount': 0.6,
            'batch_size': 256,
            'num_channels': 64,
            'replay_memory_size': 100000,
            'dropout': 0.4,
            'initial_epsilon': 0.1,
            'final_epsilon': 0.01,
            'dir': './Models/',
            'load_checkpoint': True,
            'saved_checkpoint': True
        })
]
        
def test(): 
    data = Data(cargs.min_size, cargs.max_size)
    env = Environment(data.get_random_map(), cargs.show_screen, cargs.max_size)
    agent = [Agent(env, args[0], 'agent_1'), Agent(env, args[1], 'agent_2')]
    wl_mean, score_mean = [[deque(maxlen = 10000), deque(maxlen = 10000)]  for _ in range(2)]
    wl, score = [[deque(maxlen = 1000), deque(maxlen = 1000)] for _ in range(2)]
    cnt_w, cnt_l = 0, 0
    # agent[0].model.load_state_dict(torch.load(checkpoint_path_1, map_location = agent[0].model.device))
    # agent[1].model.load_state_dict(torch.load(checkpoint_path_2, map_location = agent[1].model.device))
        
    for _ep in range(cargs.n_epochs):
        if _ep % 10 == 9:
            print('Testing_epochs: {}'.format(_ep + 1))
        done = False
        start = time.time()
        current_state = env.get_observation(0)
        for _iter in count():
            if cargs.show_screen:
                env.render()
                
            """ initialize """
            actions, soft_state, soft_agent_pos = [[[], []] for i in range(3)]
                
            """ update by step """
            for i in range(env.num_players):
                soft_agent_pos[i] = env.get_agent_pos(i)
                
            """ select action for each agent """
            for agent_id in range(env.n_agents):
                for i in range(env.num_players):
                    state_step = env.get_states_for_step(current_state)
                    agent_step = env.get_agent_for_step(agent_id, soft_agent_pos)
                    act, log_p, state_val = agent[i].select_action(state_step, agent_step)
                            
                    valid, current_state, reward = env.soft_step(agent_id, current_state, act, soft_agent_pos[0])
                    current_state, soft_agent_pos = env.get_opn_observation(current_state, soft_agent_pos)
                    actions[i].append(act)
                    
            # actions[1] = [np.random.randint(0, env.n_actions - 1) for _ in range(env.n_agents)]
            # actions[1] = [0] * env.n_agents
            # actions[1] = pred_acts[1]
            current_state, final_reward, done, _ = env.step(actions[0], actions[1], cargs.show_screen)
            if done:
                score[0].append(env.players[0].total_score)
                score[1].append(env.players[1].total_score)
                if env.players[0].total_score > env.players[1].total_score:
                    cnt_w += 1
                else:
                    cnt_l += 1
                break
            
        end = time.time()
            
        wl[0].append(cnt_w)
        wl[1].append(cnt_l)
        for i in range(2):
            wl_mean[i].append(np.mean(wl[i]))
            score_mean[i].append(np.mean(score[i]))
                
        if _ep % 50 == 49:
            plot(wl_mean, vtype = 'Win')
            plot(score_mean, vtype = 'Score')
            print("Time: {0: >#.3f}s". format(1000*(end - start)))
        env = Environment(data.get_random_map(), cargs.show_screen, cargs.max_size)

if __name__ == "__main__":
    test()